{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LocalForge AI - Privacy-First AI Coding Assistant\n\n---\n\n## **Main Features**\n- **DeepSeek Coder 1.3B Model**  \n  *Lightweight (3GB) and **2–3× faster** than 7B models.*\n- **T4 GPU Optimization**  \n  *Includes **8-bit quantization** for maximum efficiency.*\n- **RAG System**  \n  *Powered by **Sentence Transformers** and **Cosine Similarity Search** for intelligent retrieval.*\n- **Multi-format Support**  \n  *Handles **PDF, DOCX, TXT, CSV, and code files** seamlessly.*\n- **FastAPI Backend**  \n  *Full **REST API** for easy integration.*\n- **Thread Management**  \n  *Persistent sessions with **SQLite** storage.*\n- **File Upload & Processing**  \n  *Background **embedding generation** for smooth performance.*\n\n---\n\n","metadata":{"_cell_guid":"c8dd7cf2-b65a-4b14-9944-536352bbeecd","_uuid":"8f14ec58-a9c4-489b-95ec-00ee412dd762","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true}},{"cell_type":"code","source":"# ============================================================\n# CELL A: Install Dependencies \n# ============================================================\n\nprint(\"Installing LocalForge AI dependencies...\")\n\n# Core ML & AI libraries\n!pip install -q transformers accelerate bitsandbytes sentence-transformers\n\n# Web framework & utilities\n!pip install -q fastapi uvicorn \"python-multipart\" pyngrok\n\n# RAG Document Processing Libraries\n!pip install -q PyPDF2 python-docx pandas openpyxl\n\n# System utilities\n!pip install -q numpy torch\n\nprint(\"All dependencies installed successfully!\")","metadata":{"_cell_guid":"cell-a-dependencies","_uuid":"cell-a-dependencies","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# CELL B: Configuration & Backend Setup\n# ============================================================\nimport os\n\nos.environ[\"BACKEND_API_KEY\"] = \"key123\" # ⚠️ SECURITY: Change this API key to your own secure key!\nos.environ[\"HF_MODEL\"] = \"deepseek-ai/deepseek-coder-1.3b-instruct\"  # ⚠️ Configuration: Change this model name according to your needs!\nos.environ[\"LOCALFORGE_BASE\"] = \"/kaggle/working/LocalForgeAI\"\nos.environ[\"PORT\"] = \"8000\"\n\nprint(\" Configuration:\")\nprint(f\"   Model: {os.environ['HF_MODEL']}\")\nprint(f\"   API Key: {os.environ['BACKEND_API_KEY']}\")\nprint(f\"   Storage: {os.environ['LOCALFORGE_BASE']}\")\nprint(f\"   Port: {os.environ['PORT']}\\n\")\n\n# Create backend directory\nBACKEND_DIR = \"/kaggle/working/localforge_backend\"\nos.makedirs(BACKEND_DIR, exist_ok=True)\n\n# Write main.py backend code\nbackend_code = r'''\n# BackendCode.py\nimport os\nimport json\nimport time\nimport base64\nimport threading\nimport logging\nimport sqlite3\nimport mimetypes\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import List, Optional, Dict, Any\n\nfrom fastapi import FastAPI, UploadFile, File, Form, Header, HTTPException\nfrom fastapi.responses import FileResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\n\n# ============================================================\n# CONFIG & ENV\n# ============================================================\nAPI_KEY = os.environ.get(\"BACKEND_API_KEY\", \"test123\") # ⚠️ SECURITY: Change this API key to match your own secure key!\nMODEL_NAME = \"deepseek-ai/deepseek-coder-1.3b-instruct\" # ⚠️ Configuration: Change this model name according to your needs!\n\nTOP_K = 2\nMAX_CONTEXT_CHUNKS = 8\n\nDB_PATH = Path(\"localforge.db\")\nUPLOAD_DIR = Path(\"uploads\")\nUPLOAD_DIR.mkdir(exist_ok=True)\n# ============================================================\n# SYSTEM PROMPT\n# ============================================================\nSYSTEM_PROMPT = \"\"\"\\\nYou are LocalForge AI, a helpful coding and content assistant.\n\nCORE PRINCIPLES:\n1) Provide comprehensive, helpful answers that address the user's needs thoroughly\n2) Be creative and flexible in your approach - suggest multiple solutions or perspectives\n3) Use examples, analogies, or additional context when helpful\n4) If unsure, explain what you do know and your limitations\n5) Be conversational and friendly while remaining informative\n6) Share related knowledge or insights that might be valuable\n7) When explaining code, include comments and reasoning\n8) Explore creative solutions and think outside the box\n9) Provide practical, actionable advice\n10) Be encouraging and supportive of the user's goals\n\nRESPONSE STYLE:\n- Conversational, natural tone\n- Clear explanations with examples\n- Use markdown formatting for readability\n- Include relevant details, tips, or alternatives\n\"\"\"\n\n# ============================================================\n# LOGGER\n# ============================================================\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"localforge\")\n\n# ============================================================\n# DEPENDENCIES CHECK\n# ============================================================\ntry:\n    from sentence_transformers import SentenceTransformer\n    S2T_AVAILABLE = True\nexcept ImportError:\n    S2T_AVAILABLE = False\n\ntry:\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    import torch\n    HF_AVAILABLE = True\nexcept ImportError:\n    HF_AVAILABLE = False\n\ntry:\n    import PyPDF2\n    PDF_AVAILABLE = True\nexcept ImportError:\n    PDF_AVAILABLE = False\n\ntry:\n    import docx\n    DOCX_AVAILABLE = True\nexcept ImportError:\n    DOCX_AVAILABLE = False\n\ntry:\n    import pandas as pd\n    PANDAS_AVAILABLE = True\nexcept ImportError:\n    PANDAS_AVAILABLE = False\n\n# ============================================================\n# DATABASE\n# ============================================================\ndef get_conn():\n    conn = sqlite3.connect(DB_PATH)\n    conn.row_factory = sqlite3.Row\n    return conn\n\ndef init_db():\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS threads (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        title TEXT,\n        created_at REAL\n    )\"\"\")\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS messages (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        thread_id INTEGER,\n        role TEXT,\n        content TEXT,\n        created_at REAL\n    )\"\"\")\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS files (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        filename TEXT,\n        filepath TEXT,\n        created_at REAL\n    )\"\"\")\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS chunks (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        file_id INTEGER,\n        thread_id INTEGER,\n        doc_name TEXT,\n        chunk_index INTEGER,\n        text TEXT,\n        vector TEXT,\n        created_at REAL\n    )\"\"\")\n    cur.execute(\"\"\"\n    CREATE TABLE IF NOT EXISTS embeddings (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        doc_name TEXT,\n        content TEXT,\n        vector TEXT,\n        created_at REAL\n    )\"\"\")\n    conn.commit()\n    conn.close()\n\ninit_db()\n\n# ============================================================\n# UTILS\n# ============================================================\ndef require_api_key(x_api_key: Optional[str]):\n    if x_api_key != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n\ndef cosine_sim(a: List[float], b: List[float]) -> float:\n    import numpy as np\n    if not a or not b:\n        return 0.0\n    a = np.array(a)\n    b = np.array(b)\n    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))\n\ndef is_clean_text(s: str) -> bool:\n    \"\"\"Heuristic to skip binary-like / metadata chunks.\"\"\"\n    if not s or len(s.strip()) < 20:\n        return False\n    # Reject lines with high proportion of non-letter ASCII (xref tables, object markers)\n    letters = sum(ch.isalpha() for ch in s)\n    nonprint = sum((ord(ch) < 32 and ch not in \"\\n\\t\\r\") for ch in s)\n    if nonprint > 0:\n        return False\n    density = letters / max(len(s), 1)\n    # Accept if some letters and density is reasonable\n    return letters >= 30 and density >= 0.15\n\ndef clean_text(s: str) -> str:\n    \"\"\"Remove typical PDF artifacts.\"\"\"\n    bad_tokens = (\"xref\", \"endobj\", \"obj\", \"/XObject\", \"/Subtype\", \"/Image\")\n    lines = []\n    for ln in s.splitlines():\n        lns = ln.strip()\n        if any(tok in lns for tok in bad_tokens):\n            continue\n        lines.append(lns)\n    return \"\\n\".join(lines)\n\n# ============================================================\n# EMBEDDINGS\n# ============================================================\n_embed_model = None\n_embed_lock = threading.Lock()\n\ndef get_embed_model():\n    global _embed_model\n    if not S2T_AVAILABLE:\n        return None\n    with _embed_lock:\n        if _embed_model is None:\n            _embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n    return _embed_model\n\ndef embed_text(texts: List[str]) -> List[List[float]]:\n    model = get_embed_model()\n    if model:\n        return model.encode(texts).tolist()\n    return [[0.0] * 384 for _ in texts]\n\n# ============================================================\n# MODEL LOAD & GENERATION\n# ============================================================\n_model = None\n_tokenizer = None\n_model_lock = threading.Lock()\n\ndef load_hf_model_async():\n    global _model, _tokenizer\n    if not HF_AVAILABLE:\n        logger.info(\"Transformers not available; generation disabled\")\n        return\n    with _model_lock:\n        if _model is None:\n            try:\n                _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n                _model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True, device_map=\"auto\")\n                logger.info(\"HF model loaded\")\n            except Exception as e:\n                logger.exception(\"Failed to load HF model: %s\", e)\n\nthreading.Thread(target=load_hf_model_async, daemon=True).start()\n\ndef extract_after_response_marker(text: str) -> str:\n    \"\"\"Return text after '### Response:' marker; if missing, return original.\"\"\"\n    marker = \"### Response:\"\n    if marker in text:\n        return text.split(marker, 1)[1].strip()\n    return text.strip()\n\ndef model_generate(prompt: str, max_tokens: int = 256, temperature: float = 0.7) -> str:\n    if _model is None or _tokenizer is None:\n        # Even if model missing, return a graceful message\n        return \"Model unavailable on this server. Try Demo Mode or enable Transformers.\"\n    try:\n        inputs = _tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n        device = next(_model.parameters()).device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = _model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=temperature,\n            top_p=0.95,\n            do_sample=(temperature > 0.0),\n            pad_token_id=_tokenizer.eos_token_id\n        )\n        text = _tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Prefer extracting after marker to avoid echoing system prompt\n        return extract_after_response_marker(text)\n    except Exception as e:\n        logger.exception(\"Generation error\")\n        return f\"Generation error: {e}\"\n\n# ============================================================\n# DOCUMENT PROCESSING\n# ============================================================\ndef process_document_content(raw: bytes, filename: str) -> str:\n    name = filename.lower()\n\n    if name.endswith(\".pdf\") and PDF_AVAILABLE:\n        try:\n            reader = PyPDF2.PdfReader(BytesIO(raw))\n            pages = []\n            for page in reader.pages:\n                txt = page.extract_text() or \"\"\n                txt = clean_text(txt)\n                if is_clean_text(txt):\n                    pages.append(txt)\n            return \"\\n\\n\".join(pages)\n        except Exception:\n            # Don't return raw binary; return empty text to avoid useless chunks\n            return \"\"\n\n    if name.endswith(\".docx\") and DOCX_AVAILABLE:\n        try:\n            doc = docx.Document(BytesIO(raw))\n            text = \"\\n\".join(p.text for p in doc.paragraphs)\n            text = clean_text(text)\n            return text if is_clean_text(text) else \"\"\n        except Exception:\n            return \"\"\n\n    # Text-like fallback\n    try:\n        text = raw.decode(errors=\"ignore\")\n        text = clean_text(text)\n        return text if is_clean_text(text) else \"\"\n    except Exception:\n        return \"\"\n\ndef chunk_text(text: str, chunk_size: int = 1300, overlap: int = 200) -> List[str]:\n    if not text:\n        return []\n    chunks = []\n    i = 0\n    while i < len(text):\n        ch = text[i:i + chunk_size]\n        ch = clean_text(ch)\n        if is_clean_text(ch):\n            chunks.append(ch)\n        i += max(chunk_size - overlap, 1)\n    return chunks\n\n# ============================================================\n# RAG RETRIEVAL\n# ============================================================\ndef get_top_k_chunks_for_query(query: str, k: int = TOP_K, file_ids: Optional[List[int]] = None) -> List[Dict[str, Any]]:\n    if not S2T_AVAILABLE:\n        return []\n    try:\n        qvec = embed_text([query])[0]\n    except Exception:\n        return []\n\n    conn = get_conn()\n    cur = conn.cursor()\n\n    if file_ids:\n        placeholders = \",\".join(\"?\" for _ in file_ids)\n        rows = cur.execute(\n            f\"SELECT id, file_id, doc_name, chunk_index, text, vector FROM chunks WHERE file_id IN ({placeholders}) AND vector IS NOT NULL\",\n            tuple(file_ids)\n        ).fetchall()\n    else:\n        rows = cur.execute(\"SELECT id, file_id, doc_name, chunk_index, text, vector FROM chunks WHERE vector IS NOT NULL\").fetchall()\n\n    candidates = []\n    for r in rows:\n        txt = r[\"text\"] or \"\"\n        if not is_clean_text(txt):\n            continue\n        try:\n            vec = json.loads(r[\"vector\"])\n            score = cosine_sim(qvec, vec)\n        except Exception:\n            score = 0.0\n        candidates.append({\n            \"id\": r[\"id\"],\n            \"file_id\": r[\"file_id\"],\n            \"doc_name\": r[\"doc_name\"],\n            \"chunk_index\": r[\"chunk_index\"],\n            \"text\": txt,\n            \"score\": score\n        })\n    conn.close()\n\n    # Sort & cap\n    candidates = sorted(candidates, key=lambda x: x[\"score\"], reverse=True)[:k]\n    return candidates\n\ndef build_context_from_chunks(chunks: List[Dict[str, Any]]) -> str:\n    parts = []\n    for c in chunks:\n        header = f\"--- Chunk (file: {c.get('doc_name','unknown')}, idx: {c.get('chunk_index')}, score: {c.get('score',0):.4f}) ---\"\n        parts.append(header)\n        parts.append(c.get('text', ''))\n    return \"\\n\\n\".join(parts)\n\ndef conversation_history_for_thread(cur, thread_id: int, limit: int = 12) -> str:\n    cur.execute(\"SELECT role, content FROM messages WHERE thread_id=? ORDER BY created_at DESC LIMIT ?\", (thread_id, limit))\n    rows = cur.fetchall()\n    parts = [f\"{r['role']}: {r['content']}\" for r in rows[::-1]]\n    return \"\\n\".join(parts)\n\ndef build_prompt(user_prompt: str, context: Optional[str] = None) -> str:\n    parts = [SYSTEM_PROMPT]\n    if context:\n        parts.append(context)\n    parts.append(f\"### Instruction:\\n{user_prompt}\\n\\n### Response:\")\n    return \"\\n\\n\".join(parts)\n\n# ============================================================\n# FASTAPI APP\n# ============================================================\napp = FastAPI(title=\"LocalForge AI Backend - RAG One-file\", version=\"5.3.0\")\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# ============================================================\n# MODELS\n# ============================================================\nclass ChatRequest(BaseModel):\n    thread_id: Optional[int] = None\n    prompt: str\n    max_tokens: Optional[int] = 512\n    temperature: Optional[float] = 0.0\n    use_rag: Optional[bool] = True\n    attached_files: Optional[List[Dict[str, Any]]] = []\n    file_ids: Optional[List[int]] = None\n\nclass FileChatRequest(BaseModel):\n    thread_id: Optional[int] = None\n    prompt: str\n    file_ids: List[int]\n    max_tokens: Optional[int] = 512\n    temperature: Optional[float] = 0.0\n    use_rag: Optional[bool] = True\n    attached_files: Optional[List[Dict[str, Any]]] = []\n\n# ============================================================\n# ENDPOINTS\n# ============================================================\n@app.get(\"/\")\ndef root():\n    return {\n        \"status\": \"online\",\n        \"backend\": \"LocalForge AI One-file RAG\",\n        \"model\": MODEL_NAME,\n        \"version\": \"5.3.0\",\n        \"features\": [\"embedding-rag\", \"chunking\", \"background-embeddings\", \"filechat\", \"top-k-retrieval\"],\n        \"dependencies\": {\n            \"transformers\": HF_AVAILABLE,\n            \"sentence-transformers\": S2T_AVAILABLE,\n            \"pyPDF2\": PDF_AVAILABLE,\n            \"python-docx\": DOCX_AVAILABLE,\n            \"pandas\": PANDAS_AVAILABLE\n        }\n    }\n\n@app.get(\"/api/health\")\ndef health(x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"SELECT COUNT(*) as cnt FROM chunks\")\n    chunk_count = cur.fetchone()[\"cnt\"]\n    conn.close()\n    hf_loaded = True if not HF_AVAILABLE else (_model is not None and _tokenizer is not None)\n    return {\n        \"status\": \"ok\",\n        \"timestamp\": time.time(),\n        \"hf_model_loaded\": hf_loaded,\n        \"embeddings\": S2T_AVAILABLE,\n        \"chunk_count\": chunk_count\n    }\n\n# --- Threads ---\n@app.post(\"/api/threads\")\ndef create_thread(title: str = Form(\"New Conversation\"), x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"INSERT INTO threads (title, created_at) VALUES (?,?)\", (title, time.time()))\n    tid = cur.lastrowid\n    conn.commit()\n    conn.close()\n    return {\"id\": tid, \"title\": title, \"created_at\": time.time()}\n\n@app.get(\"/api/threads\")\ndef list_threads(x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM threads ORDER BY created_at DESC\")\n    rows = [dict(r) for r in cur.fetchall()]\n    conn.close()\n    return rows\n\n@app.get(\"/api/threads/{thread_id}\")\ndef get_thread(thread_id: int, x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"SELECT * FROM threads WHERE id=?\", (thread_id,))\n    thread = cur.fetchone()\n    if not thread:\n        raise HTTPException(status_code=404, detail=\"Thread not found\")\n    cur.execute(\"SELECT * FROM messages WHERE thread_id=? ORDER BY created_at\", (thread_id,))\n    messages = [dict(r) for r in cur.fetchall()]\n    conn.close()\n    return {\"thread\": dict(thread), \"messages\": messages}\n\n# ============================================================\n# FILE MANAGEMENT\n# ============================================================\n@app.post(\"/api/upload\")\nasync def upload_file(file: UploadFile = File(...), x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    ts = int(time.time())\n    stored_name = f\"{ts}_{file.filename}\"\n    stored_path = str(UPLOAD_DIR / stored_name)\n\n    contents = await file.read()\n    with open(stored_path, \"wb\") as fh:\n        fh.write(contents)\n\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"INSERT INTO files (filename, filepath, created_at) VALUES (?,?,?)\", (file.filename, stored_path, time.time()))\n    file_id = cur.lastrowid\n    conn.commit()\n\n    # process text and chunks (skip junk)\n    try:\n        text = process_document_content(contents, file.filename)\n        chunks = chunk_text(text)\n        for idx, chunk in enumerate(chunks):\n            cur.execute(\n                \"INSERT INTO chunks (file_id, doc_name, chunk_index, text, vector, created_at) VALUES (?,?,?,?,?,?)\",\n                (file_id, file.filename, idx, chunk, None, time.time())\n            )\n        conn.commit()\n        inserted_chunks = len(chunks)\n    except Exception:\n        logger.exception(\"Failed to extract/chunk uploaded file\")\n        inserted_chunks = 0\n    conn.close()\n\n    # spawn background embedding if possible\n    if S2T_AVAILABLE and inserted_chunks:\n        def bg_job():\n            try:\n                c = get_conn()\n                cur2 = c.cursor()\n                rows = cur2.execute(\"SELECT id, text FROM chunks WHERE file_id=? ORDER BY chunk_index\", (file_id,)).fetchall()\n                for r in rows:\n                    vec = embed_text([r[\"text\"]])[0]\n                    cur2.execute(\"UPDATE chunks SET vector=? WHERE id=?\", (json.dumps(vec), r[\"id\"]))\n                c.commit()\n                c.close()\n            except Exception:\n                logger.exception(\"Background embedding job failed\")\n        threading.Thread(target=bg_job, daemon=True).start()\n\n    return {\"id\": file_id, \"filename\": file.filename, \"text_extracted\": inserted_chunks > 0, \"embedding_job\": \"started\" if inserted_chunks else \"none\"}\n\n@app.get(\"/api/files\")\ndef list_files(x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"SELECT id, filename, created_at FROM files ORDER BY created_at DESC\")\n    rows = [dict(r) for r in cur.fetchall()]\n    conn.close()\n    return rows\n\n@app.get(\"/api/files/{file_id}/content\")\ndef get_file_content(file_id: int, x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    row = cur.execute(\"SELECT filename, filepath FROM files WHERE id=?\", (file_id,)).fetchone()\n    conn.close()\n\n    if not row:\n        raise HTTPException(status_code=404, detail=\"File not found\")\n\n    filename, filepath = row[\"filename\"], row[\"filepath\"]\n    if not os.path.exists(filepath):\n        raise HTTPException(status_code=404, detail=\"File missing on disk\")\n\n    ext = Path(filepath).suffix.lower()\n    if ext in [\".txt\", \".md\", \".py\", \".json\", \".csv\", \".html\", \".xml\"]:\n        with open(filepath, \"rb\") as fh:\n            return fh.read().decode(errors=\"ignore\")\n\n    return FileResponse(filepath, media_type=mimetypes.guess_type(filepath)[0] or \"application/octet-stream\", filename=filename)\n\n@app.post(\"/api/files/{file_id}/delete\")\ndef delete_file(file_id: int, x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n    cur.execute(\"SELECT filepath FROM files WHERE id=?\", (file_id,))\n    row = cur.fetchone()\n    if row and os.path.exists(row[\"filepath\"]):\n        try:\n            os.remove(row[\"filepath\"])\n        except Exception:\n            logger.exception(\"Failed to remove file from disk\")\n    cur.execute(\"DELETE FROM chunks WHERE file_id=?\", (file_id,))\n    cur.execute(\"DELETE FROM files WHERE id=?\", (file_id,))\n    conn.commit()\n    conn.close()\n    return {\"deleted_file_id\": file_id}\n\n# ============================================================\n# RAG & CHAT\n# ============================================================\n@app.post(\"/api/chat\")\ndef chat(req: ChatRequest, x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n\n    # Create thread if missing\n    if not req.thread_id:\n        cur.execute(\"INSERT INTO threads (title, created_at) VALUES (?,?)\", (req.prompt[:80], time.time()))\n        req.thread_id = cur.lastrowid\n\n    # Store user message\n    cur.execute(\"INSERT INTO messages (thread_id, role, content, created_at) VALUES (?,?,?,?)\",\n                (req.thread_id, \"user\", req.prompt, time.time()))\n    conn.commit()\n\n    conv_ctx = conversation_history_for_thread(cur, req.thread_id, limit=6)\n\n    combined_chunks: List[Dict[str, Any]] = []\n    \n    if S2T_AVAILABLE:\n        try:\n            combined_chunks = get_top_k_chunks_for_query(req.prompt, k=TOP_K, file_ids=req.file_ids)\n        except Exception:\n            logger.exception(\"filechat retrieval failed\")\n\n\n    # attach files (base64) if provided\n    if req.attached_files:\n        try:\n            qvec = embed_text([req.prompt])[0] if S2T_AVAILABLE else None\n        except Exception:\n            qvec = None\n        for f in req.attached_files:\n            raw = base64.b64decode(f.get(\"content\", \"\"))\n            text = process_document_content(raw, f.get(\"name\", \"attached\"))\n            chs = chunk_text(text)\n            if S2T_AVAILABLE and qvec is not None:\n                vecs = embed_text(chs)\n                for idx, (txt, vec) in enumerate(zip(chs, vecs)):\n                    score = cosine_sim(qvec, vec)\n                    combined_chunks.append({\"id\": None, \"file_id\": None, \"doc_name\": f.get(\"name\", \"attached\"),\n                                            \"chunk_index\": idx, \"text\": txt, \"score\": score})\n            else:\n                for idx, txt in enumerate(chs):\n                    combined_chunks.append({\"id\": None, \"file_id\": None, \"doc_name\": f.get(\"name\", \"attached\"),\n                                            \"chunk_index\": idx, \"text\": txt, \"score\": 0.0})\n\n    # STRONG DEDUPE after merging\n    dedup_map: Dict[tuple, Dict[str, Any]] = {}\n    for c in combined_chunks:\n        key = (c.get(\"id\"), c.get(\"doc_name\"), c.get(\"chunk_index\"))\n        # prefer higher score if duplicates occur\n        if key not in dedup_map or (c.get(\"score\", 0) > dedup_map[key].get(\"score\", 0)):\n            dedup_map[key] = c\n    combined_chunks = sorted(dedup_map.values(), key=lambda x: x.get(\"score\", 0), reverse=True)[:MAX_CONTEXT_CHUNKS]\n\n    context_text = build_context_from_chunks(combined_chunks) if combined_chunks else \"\"\n    full_context_parts = []\n    if conv_ctx.strip():\n        full_context_parts.append(\"### Conversation History:\\n\" + conv_ctx)\n    if context_text.strip():\n        full_context_parts.append(\"### Retrieved Documents:\\n\" + context_text)\n    final_context = \"\\n\\n\".join(full_context_parts).strip() if full_context_parts else None\n\n    final_prompt = build_prompt(req.prompt, context=final_context)\n    reply = model_generate(final_prompt, max_tokens=req.max_tokens or 512, temperature=req.temperature or 0.0)\n\n    # Store assistant reply\n    cur.execute(\"INSERT INTO messages (thread_id, role, content, created_at) VALUES (?,?,?,?)\",\n                (req.thread_id, \"assistant\", reply, time.time()))\n    conn.commit()\n    conn.close()\n\n    return {\n        \"thread_id\": req.thread_id,\n        \"answer\": reply,\n        \"rag_used\": bool(combined_chunks),\n        \"files_processed\": len(set(c.get(\"file_id\") for c in combined_chunks if c.get(\"file_id\") is not None)),\n        \"files_attached\": len(req.attached_files) if req.attached_files else 0,\n        \"rag_details\": f\"Retrieved {len(combined_chunks)} clean chunks\"\n    }\n\n@app.post(\"/api/filechat\")\ndef filechat(req: FileChatRequest, x_api_key: Optional[str] = Header(None)):\n    require_api_key(x_api_key)\n    conn = get_conn()\n    cur = conn.cursor()\n\n    # Create thread if missing\n    if not req.thread_id:\n        cur.execute(\"INSERT INTO threads (title, created_at) VALUES (?,?)\", (req.prompt[:80], time.time()))\n        req.thread_id = cur.lastrowid\n\n    # Store user message\n    cur.execute(\"INSERT INTO messages (thread_id, role, content, created_at) VALUES (?,?,?,?)\",\n                (req.thread_id, \"user\", req.prompt, time.time()))\n    conn.commit()\n\n    # Conversation history\n    conv_ctx = conversation_history_for_thread(cur, req.thread_id, limit=6)\n\n    # Extract text from attached files (no embeddings)\n    file_texts = []\n    if req.attached_files:\n        for f in req.attached_files:\n            raw = base64.b64decode(f.get(\"content\", \"\"))\n            text = process_document_content(raw, f.get(\"name\", \"attached\"))\n            file_texts.append(text)\n\n    combined_file_text = \"\\n\\n\".join(file_texts).strip() if file_texts else \"\"\n\n    # Build final prompt: user prompt + conversation history + extracted file text\n    full_context_parts = []\n    if conv_ctx.strip():\n        full_context_parts.append(\"### Conversation History:\\n\" + conv_ctx)\n    if combined_file_text:\n        full_context_parts.append(\"### Attached File Content:\\n\" + combined_file_text)\n    \n    final_context = \"\\n\\n\".join(full_context_parts).strip() if full_context_parts else None\n    final_prompt = build_prompt(req.prompt, context=final_context)\n\n    # Generate model reply\n    reply = model_generate(final_prompt, max_tokens=req.max_tokens or 512, temperature=req.temperature or 0.0)\n\n    # Store assistant reply\n    cur.execute(\"INSERT INTO messages (thread_id, role, content, created_at) VALUES (?,?,?,?)\",\n                (req.thread_id, \"assistant\", reply, time.time()))\n    conn.commit()\n    conn.close()\n\n    return {\n        \"thread_id\": req.thread_id,\n        \"answer\": reply,\n        \"files_attached\": len(req.attached_files) if req.attached_files else 0\n    }\n\n# Startup log\nlogger.info(\"LocalForge AI One-file RAG backend ready. DB: %s, Upload dir: %s\", DB_PATH, UPLOAD_DIR)\n'''\n\nwith open(f\"{BACKEND_DIR}/main.py\", \"w\") as f:\n    f.write(backend_code)\n\nprint(f\"\\n Backend code written to {BACKEND_DIR}/main.py\")\nprint(f\" File size: {len(backend_code)} bytes\\n\")","metadata":{"_cell_guid":"cell-b-setup","_uuid":"cell-b-setup","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# CELL C: Start Server with Tunnel and Capture Output\n# ============================================================\n\nimport os\nimport subprocess\nimport time\nimport sys\nimport threading\nimport requests\nfrom pyngrok import ngrok\nimport torch\n\n# Move to backend directory\nBACKEND_DIR = \"/kaggle/working/localforge_backend\"\nif os.path.exists(BACKEND_DIR):\n    os.chdir(BACKEND_DIR)\nelse:\n    print(f\"Warning: Directory {BACKEND_DIR} not found. Running in current dir.\")\n\nPORT = os.environ.get(\"PORT\", \"8000\")\nAPI_KEY = os.environ.get(\"BACKEND_API_KEY\", \"test_key\") \nHF_MODEL = os.environ.get(\"HF_MODEL\", \"default_model\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\" STARTING LocalForge AI SERVER\")\nprint(\"=\"*70)\n\n# Configure ngrok\nNGROK_TOKEN = \"PlaceHolder\"   # ⚠️ Configuration: Change this token to your own NGROK Token!\nngrok.set_auth_token(NGROK_TOKEN)\n\n# Check GPU\nif torch.cuda.is_available():\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\" Memory: {gpu_memory:.2f} GB\")\n    print(f\" T4 optimizations: ACTIVE\")\nelse:\n    print(\"  No GPU detected\")\n\nprint(f\"\\n Model: {HF_MODEL}\")\nprint(f\" Port: {PORT}\")\n\n# Start FastAPI server\nprint(f\"\\n Starting FastAPI server...\\n\")\nuvicorn_proc = subprocess.Popen(\n    [\"python3\", \"-m\", \"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", PORT],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True,\n    bufsize=1 \n)\n\n# Stream output function\ndef stream_output(proc):\n    try:\n        # Iterate over stdout line by line\n        for line in iter(proc.stdout.readline, \"\"):\n            print(line, end=\"\")\n            sys.stdout.flush() # Force Kaggle to print immediately\n    except Exception as e:\n        print(f\" Output streaming error: {e}\")\n\n# Start the logging thread\nlog_thread = threading.Thread(target=stream_output, args=(uvicorn_proc,), daemon=True)\nlog_thread.start()\n\n# Wait for server to initialize\ntime.sleep(5)\n\n# Start ngrok tunnel\ntry:\n    public_url = ngrok.connect(PORT)\n    print(f\"\\n\" + \"=\"*70)\n    print(\"  LocalForge AI Server IS READY!\")\n    print(\"=\"*70)\n    print(f\"\\n Copy this URL for your VS Code extension:\")\n    print(f\" Public URL: {public_url}\")\n    print(f\" API Key: {API_KEY}\")\n    print(\"\\n Keep this cell running!\")\n    print(\"=\"*70)\nexcept Exception as e:\n    print(f\"Ngrok error: {e}\")\n\n# Health check function\ndef check_server_health(retries=5, delay=3):\n    print(\"\\n Performing Health Check...\")\n    for i in range(retries):\n        try:\n            response = requests.get(\n                f\"http://localhost:{PORT}/api/health\",\n                headers={\"x-api-key\": API_KEY},\n                timeout=10\n            )\n            if response.status_code == 200:\n                health = response.json()\n                print(f\" Health Check Passed!\")\n                print(f\"   Status: {health.get('status')}\")\n                print(f\"   Model Loaded: {health.get('hf_model_loaded')}\")\n                return\n            else:\n                print(f\" Health check failed: {response.status_code}\")\n        except Exception as e:\n            pass \n        time.sleep(delay)\n    print(\"  Health check failed after retries (Server might still be loading model)\")\n\ncheck_server_health()\n\n# ============================================================\n# BLOCKING LOOP\n# ============================================================\nprint(\"\\nLogs are streaming below. Stop cell to terminate server.\\n\")\n\ntry:\n    while uvicorn_proc.poll() is None:\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    print(\"Stopping server...\")\n    uvicorn_proc.terminate()\n    ngrok.kill()\n    print(\"Server stopped.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}